{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion\n",
    "### Using FastText Word Embedding\n",
    "Based on this paper: https://arxiv.org/pdf/1606.07608.pdf\n",
    "\n",
    "Pre-made vector models: https://fasttext.cc/docs/en/aligned-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:16.676369Z",
     "start_time": "2019-06-12T13:31:15.880009Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# import natural language toolkit\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# prepare stopword list\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T07:16:14.874574Z",
     "start_time": "2019-06-12T07:06:52.184229Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_en_align = './data/fasttext/wiki.en.align.vec' #change directory to old one for marquis\n",
    "# get fasttext wiki embeddings for english\n",
    "wv_wiki_en = KeyedVectors.load_word2vec_format(wiki_en_align)\n",
    "print('english words {}'.format(len(list(wv_wiki_en.vocab.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-retrieval kNN Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:17.961109Z",
     "start_time": "2019-06-12T13:31:17.954389Z"
    }
   },
   "outputs": [],
   "source": [
    "#list of terms\n",
    "def tokenize(text, stopwords):\n",
    "    \"\"\"Tokenizes and removes stopwords from the document\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [w.lower() for w in tokens if not w in stopwords]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extended list of terms ###\n",
    "def extend_tokens(token_list, wv):\n",
    "    \"\"\"Extends token list summing vector pairs\"\"\"\n",
    "    tokens = []\n",
    "    for token in token_list:\n",
    "        # check if the token is in the vocabulary\n",
    "        if token in wv.vocab.keys():\n",
    "            tokens.append(token)\n",
    "    extention = set()\n",
    "    for i in range(len(tokens)-1):\n",
    "        new_token = wv_wiki_en.most_similar(positive=[tokens[i], tokens[i+1]])[0][0]\n",
    "        extention.add(new_token)\n",
    "    extention = list(extention)\n",
    "    return extention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenize('water pollution underground', stop_words)\n",
    "print(test)\n",
    "ext = extend_tokens(test,wv_wiki_en)\n",
    "print(ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tokenize('annex fishing agreement europe', stop_words)\n",
    "print(test1)\n",
    "ext1 = extend_tokens(test1,wv_wiki_en)\n",
    "print(ext1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:18.473214Z",
     "start_time": "2019-06-12T13:31:18.465157Z"
    }
   },
   "outputs": [],
   "source": [
    "# knn nearest\n",
    "def get_candidate_expansion_terms(tokens, k, wv):\n",
    "    \"\"\"Gets the candidate expansion terms\"\"\"\n",
    "    candidates = set()\n",
    "    for token in tokens:\n",
    "        # check if the token is in the vocabulary\n",
    "        if token in wv.vocab.keys():\n",
    "            result = wv.similar_by_word(token)\n",
    "            limit = k if len(result) > k else len(result)\n",
    "            # iterate through the most similar words\n",
    "            for i in range(limit):\n",
    "                candidates.add(result[i][0])\n",
    "    # return list of candidates\n",
    "    candidates = list(candidates)\n",
    "    return candidates\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:20.226899Z",
     "start_time": "2019-06-12T13:31:19.569959Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = get_candidate_expansion_terms(test+ext, 5, wv_wiki_en)\n",
    "print(candidates)\n",
    "witout = get_candidate_expansion_terms(test, 5, wv_wiki_en)\n",
    "print(witout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between word and list of words\n",
    "def similarity(token, token_list, wv ):\n",
    "    \"\"\"calculates the similarity between word and list of words\"\"\"\n",
    "    # calculate the similarity of the token to all tokens\n",
    "    similarity = 0\n",
    "    num_of_tokens = 0\n",
    "    for toks in token_list:\n",
    "        # check if the token is in the vocabulary\n",
    "        if toks in wv.vocab.keys():\n",
    "            num_of_tokens += 1\n",
    "            similarity += wv.similarity(toks, token)\n",
    "    return similarity/num_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:13.725242Z",
     "start_time": "2019-06-12T13:33:13.716880Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculates similarity and sorts\n",
    "def get_top_expansion_terms(tokens, candidates, wv):\n",
    "    \"\"\"Gets the actual expansion terms\"\"\"\n",
    "    similarity_pairs = []\n",
    "    for candidate in candidates:\n",
    "        sim = similarity(candidate, tokens, wv)\n",
    "        similarity_pairs.append((candidate, sim))\n",
    "    # return the list of expansion terms with their similarities\n",
    "    return similarity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:14.317627Z",
     "start_time": "2019-06-12T13:33:14.308832Z"
    }
   },
   "outputs": [],
   "source": [
    "top = get_top_expansion_terms(test+ext, candidates,  wv_wiki_en)\n",
    "topwithout = get_top_expansion_terms(test, candidates,  wv_wiki_en)\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "top = sorted(top, key=takeSecond)[::-1]\n",
    "topw = sorted(topwithout, key=takeSecond)[::-1]\n",
    "print((top))\n",
    "print((topw))\n",
    "top = top[0:5]\n",
    "topw = topw[0:5]\n",
    "top_list = []\n",
    "for tupl in top:\n",
    "    top_list.append(tupl[0])\n",
    "topw_list = []\n",
    "for tupl in topw:\n",
    "    topw_list.append(tupl[0])\n",
    "\n",
    "top1 = get_top_expansion_terms(test1+ext1, candidates,  wv_wiki_en)\n",
    "topwithout1 = get_top_expansion_terms(test1, candidates,  wv_wiki_en)\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "top1 = sorted(top1, key=takeSecond)[::-1]\n",
    "topw1 = sorted(topwithout1, key=takeSecond)[::-1]\n",
    "top1 = top1[0:5]\n",
    "topw1 = topw1[0:5]\n",
    "top_list1 = []\n",
    "for tupl in top1:\n",
    "    top_list1.append(tupl[0])\n",
    "topw_list1 = []\n",
    "for tupl in topw1:\n",
    "    topw_list1.append(tupl[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:17.453260Z",
     "start_time": "2019-06-12T13:33:17.445772Z"
    }
   },
   "outputs": [],
   "source": [
    "# all functions together, finds k nearest for each term, returns top n\n",
    "def pre_retrieval_KNN(string, k, wv, n):\n",
    "    \"\"\"Find the most similar tokens to the given query\"\"\"\n",
    "    tokens = tokenize(string, stop_words)\n",
    "    candidates = get_candidate_expansion_terms(tokens, k, wv)\n",
    "    candidates_sim = get_top_expansion_terms(tokens, candidates, wv)\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "    sort = sorted(candidates_sim, key=takeSecond)[::-1]\n",
    "    return sort[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:18.114793Z",
     "start_time": "2019-06-12T13:33:17.927772Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_retrieval_KNN('deep learning', 5, wv_wiki_en, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import postgresql\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from modules.library.postgresql import PostgresQL\n",
    "# connect to the postgresql database\n",
    "pg = PostgresQL() \n",
    "pg.connect(database=\"eurlex_environment_only\", user=\"postgres\", password=\"dbpass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import documents \n",
    "documents = pg.execute(\"\"\"\n",
    "    SELECT * FROM documents;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "documents[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = documents\n",
    "# some docs are empty !!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# for doc in docs:\n",
    "#     idd= doc.get('document_id')\n",
    "#     if idd == 39722:\n",
    "#         print(doc)\n",
    "\n",
    "delete = []\n",
    "for doc in docs:\n",
    "    n = len(doc.get('document_text'))\n",
    "    if n == 0:\n",
    "        id_doc = doc.get('document_id')\n",
    "        delete.append(id_doc)\n",
    "            \n",
    "# remove empty docs\n",
    "for doc in docs:\n",
    "    id_doc = doc.get('document_id')\n",
    "    if id_doc in delete:\n",
    "        docs.remove(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokenzized documents (texts), texts, tokenzized titles and titles\n",
    "tokenized_docs = {}\n",
    "tokenized_titles = {}\n",
    "texts = {}\n",
    "titles = {}\n",
    "for document in docs:\n",
    "    doc_id = document.get('document_id')\n",
    "    text = document.get('document_text')\n",
    "    texts.update({doc_id: text})\n",
    "    title = document.get('document_title')\n",
    "    titles.update({doc_id: title})\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokenized = tokenize(text, stop_words)\n",
    "    title = title.translate(str.maketrans('','',string.punctuation))\n",
    "    tokenized_title = tokenize(title, stop_words)\n",
    "    for token in tokenized:\n",
    "        if len(token) == 1:\n",
    "            if token.isalpha():\n",
    "                tokenized.remove(token)\n",
    "    tokenized_docs.update({doc_id: tokenized})\n",
    "    for title in tokenized_title:\n",
    "        if len(title) == 1:\n",
    "            if title.isalpha():\n",
    "                tokenized_title.remove(title)\n",
    "    tokenized_titles.update({doc_id: tokenized_title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tokenized_titles.get(3))\n",
    "print(titles.get(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_docs.get(39703))\n",
    "print(texts.get(39703))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_docs))\n",
    "#print(take(1, tokenized_docs.items()))\n",
    "empt=[]\n",
    "for k,v in tokenized_docs.items():\n",
    "    l =len(v)\n",
    "    if l==0:\n",
    "        empt.append(k)\n",
    "print(len(empt))\n",
    "#print((empt))\n",
    "\n",
    "for k in empt:\n",
    "    del tokenized_docs[k]\n",
    "    del tokenized_titles[k]\n",
    "    del texts[k]\n",
    "    del titles[k]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_docs))\n",
    "print(len(tokenized_titles))\n",
    "print(len(texts))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs1 = {}\n",
    "tokenized_titles1 = {}\n",
    "texts1= {}\n",
    "titles1={}\n",
    "for k in range(1000):\n",
    "    vtd = tokenized_docs.get(k)\n",
    "    vtt = tokenized_titles.get(k)\n",
    "    vx = texts.get(k)\n",
    "    vt =titles.get(k)\n",
    "    tokenized_docs1.update({k:vtd})\n",
    "    tokenized_titles1.update({k:vtt})\n",
    "    texts1.update({k:vx})\n",
    "    titles1.update({k:vt})\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_docs1))\n",
    "print(len(tokenized_titles1))\n",
    "print(len(texts1))\n",
    "print(len(titles1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### search full words (not lemmatized), search as substrings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. probability scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability scoring\n",
    "### all query words have to be in the document (multiplying)\n",
    "\n",
    "def probab_score(tokens,tokenized_docs,texts):\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 1\n",
    "        text = texts.get(k)\n",
    "        for token in tokens:\n",
    "            token_frequency = text.count(token)\n",
    "            probability = probability*(token_frequency/n)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only original query\n",
    "score =probab_score(test,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "positives = dict([(k,v) for k,v in score.items() if v > 0])\n",
    "sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_positives_top = sorted_positives[0:10]\n",
    "print(sorted_positives_top)\n",
    "\n",
    "# original query + extension\n",
    "score =probab_score(test+ext,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "print(([(k,v) for k,v in score.items() if v > 0]))\n",
    "\n",
    "\n",
    "# only original query\n",
    "score =probab_score(test1,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "positives = dict([(k,v) for k,v in score.items() if v > 0])\n",
    "sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_positives_top = sorted_positives[0:10]\n",
    "print(sorted_positives_top)\n",
    "\n",
    "# original query + extension\n",
    "score =probab_score(test1+ext1,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "print(([(k,v) for k,v in score.items() if v > 0]))\n",
    "\n",
    "\n",
    "# no point having an extention, empty results are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no point having an extention, empty results are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(titles.get(80))\n",
    "# print(texts.get(80))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query words summation\n",
    "def probab_score_sum(tokens,tokenized_docs,texts):\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 0\n",
    "        text = texts.get(k)\n",
    "        for token in tokens:\n",
    "            token_frequency = text.count(token)\n",
    "            probability = probability+(token_frequency/n)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zavedaj se:\n",
    "\"gabla is bla2\".count(\"bla\")\n",
    "# kar pomeni da bi bilo bolje uporabiti lemmatized words! popravi!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_positives(dictionary,n):\n",
    "    \"\"\"Takes dict and returns first n tuples of k,v sorted by v\"\"\"\n",
    "    positives = {} \n",
    "    for k,v in score_sum.items():\n",
    "        if v > 0:\n",
    "            positives.update({k: v})\n",
    "    sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "    sorted_positives_top = sorted_positives[0:n]\n",
    "    return sorted_positives_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only original query\n",
    "score_sum =probab_score_sum(test,tokenized_docs,texts)\n",
    "\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "df_sum_original = pd.DataFrame(sorted_positives_top, columns =['id_sum_original', 'score'])\n",
    "df_sum_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(titles.get(565))\n",
    "# print(texts.get(565))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test+ext,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "df_sum_original_ext = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only original query\n",
    "score =probab_score(test1,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score,10)\n",
    "print(sorted_positives_top)\n",
    "\n",
    "\n",
    "\n",
    "## original query\n",
    "score_sum =probab_score_sum(test1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original1', 'score'])\n",
    "df_sum_original1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test1+ext1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "positives = dict([(k,v) for k,v in score_sum.items() if v > 0])\n",
    "sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_positives_top = sorted_positives[0:10]\n",
    "#dataframe\n",
    "df_sum_original_ext1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext1', 'score'])\n",
    "df_sum_original_ext1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding candidates\n",
    "## without weights\n",
    "# no point using multiplication\n",
    "\n",
    "# summation:\n",
    "## original query\n",
    "score_sum =probab_score_sum(test+topw_list,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_cand = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_cand', 'score'])\n",
    "\n",
    "\n",
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test+ext+top_list,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_ext_cand = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext_cand', 'score'])\n",
    "\n",
    "\n",
    "## original query\n",
    "score_sum =probab_score_sum(test1+topw_list1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_cand1', 'score'])\n",
    "\n",
    "\n",
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test1+ext1+top_list1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_ext_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext_cand1', 'score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_value(word, alpha, original_tokens, top_expansion, wv):\n",
    "    \"\"\"values word based on whether is in original token set or expanded\"\"\"\n",
    "    only_expanded = []\n",
    "    for token in top_expansion:\n",
    "        if token not in original_tokens:\n",
    "            only_expanded.append(token)\n",
    "            \n",
    "    sum_similarity = 0\n",
    "    for exp_token in only_expanded:\n",
    "            sum_similarity += similarity(exp_token,original_tokens, wv)\n",
    "        \n",
    "    if word in original_tokens:\n",
    "        value = alpha\n",
    "    else:\n",
    "        value = (1-alpha)*similarity(word, original_tokens, wv)/sum_similarity\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce ni ext zraven je so cudni rezultati, zamenja vrstni red pomembnsti med sewage in undergrounding??\n",
    "top = top[0:4]\n",
    "top_words = [i[0] for i in top]\n",
    "print(word_value(\"water\", 0.7, test+ext ,top_words, wv_wiki_en))\n",
    "print(word_value(\"sewage\", 0.7, test+ext ,top_words, wv_wiki_en))\n",
    "print(word_value(\"undergrounding\", 0.7, test+ext ,top_words, wv_wiki_en))\n",
    "print(word_value(\"biopollution\", 0.7, test+ext ,top_words, wv_wiki_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probab_score_sum_weights(original_tokens, top_expansion,tokenized_docs,texts, wv, alpha): \n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 0\n",
    "        text = texts.get(k)\n",
    "        for token in original_tokens+top_expansion:\n",
    "            token_frequency = text.count(token)\n",
    "            probability = probability+(token_frequency/n)*word_value(token, alpha, original_tokens, top_expansion, wv)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with weights\n",
    "# summation \n",
    "original_query_cand = []\n",
    "for alpha in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    ## original query\n",
    "    score_sum = probab_score_sum_weights(test, topw_list,tokenized_docs, texts, wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_cand = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_cand'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_cand.append(df_wsum_original_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## original query plus ext\n",
    "original_query_ext_cand = []\n",
    "for alpha in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    score_sum =probab_score_sum_weights(test+ext, top_list,tokenized_docs, texts, wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_ext_cand = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_ext_cand'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_ext_cand.append(df_wsum_original_ext_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sorting for each alpha\n",
    "frames =[]\n",
    "for i in range(len(original_query_cand)):\n",
    "    frst = original_query_cand[i].take([0], axis=1)\n",
    "    snd = original_query_ext_cand[i].take([0], axis=1)\n",
    "    con = pd.concat([frst,snd], axis=1)\n",
    "    frames.append(con)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sorting for different alphas, original + cand\n",
    "frames =[]\n",
    "for i in range(len(original_query_cand)):\n",
    "    dataf = original_query_cand[i].take([0], axis=1)\n",
    "    frames.append(dataf)\n",
    "con = pd.concat(frames, axis=1)\n",
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first 4 rows same values\n",
    "- 5.,6.,7. row only values for alpha 0.5 different\n",
    "- lower than 7. row: values for 0.5 and 0.6 different\n",
    "- half of values lower than 4.place on alpha 0.5 also appear in other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1:\n",
    "## original query\n",
    "original_query_cand1 = []\n",
    "for alpha in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    score_sum = probab_score_sum_weights(test1, topw_list1,tokenized_docs,texts,  wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_cand1'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_cand1.append(df_wsum_original_cand1)\n",
    "\n",
    "\n",
    "## original query plus ext\n",
    "original_query_ext_cand1 = []\n",
    "for alpha in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    score_sum =probab_score_sum_weights(test1+ext1, top_list1,tokenized_docs,texts,  wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_ext_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_ext_cand1'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_ext_cand1.append(df_wsum_original_ext_cand1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sorting for each alpha\n",
    "frames1 =[]\n",
    "for i in range(len(original_query_cand1)):\n",
    "    frst = original_query_cand1[i].take([0], axis=1)\n",
    "    snd = original_query_ext_cand1[i].take([0], axis=1)\n",
    "    con = pd.concat([frst,snd], axis=1)\n",
    "    frames1.append(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sorting for different alphas, original + cand\n",
    "frames =[]\n",
    "for i in range(len(original_query_cand1)):\n",
    "    dataf = original_query_cand1[i].take([0], axis=1)\n",
    "    frames.append(dataf)\n",
    "con1 = pd.concat(frames, axis=1)\n",
    "con1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of summation method versions on test set\n",
    "frames = [df_sum_original[\"id_sum_original\"], df_sum_original_cand[\"id_sum_original_cand\"], df_sum_original_ext['id_sum_original_ext'],df_sum_original_ext_cand['id_sum_original_ext_cand'],df_wsum_original_cand['id_wsum_original_cand'], df_wsum_original_ext_cand['id_wsum_original_ext_cand']]\n",
    "sum_result = pd.concat(frames, axis=1)\n",
    "sum_result\n",
    "# error ker wsum samo se z alphami e.g. wsum0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = sum_result.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "counter=collections.Counter(flat_vals)\n",
    "print((counter))\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same results for sum original and sum original_ext\n",
    "- slight diff. between original cand and original ext cand in wsum  \n",
    " --> ext impacts on cand  \n",
    "- for top 5 only difference between using cand or not\n",
    "- 5. and 6. place id different wether ext is used or not\n",
    "- 9. place differs in column sum original cand\n",
    "- 8 values occures in all cases, so max 2 differ for each list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of summation method versions on test1 set\n",
    "frames = [df_sum_original1[\"id_sum_original1\"], df_sum_original_cand1[\"id_sum_original_cand1\"], df_sum_original_ext1['id_sum_original_ext1'],df_sum_original_ext_cand1['id_sum_original_ext_cand1'],df_wsum_original_cand1['id_wsum_original_cand1'], df_wsum_original_ext_cand1['id_wsum_original_ext_cand1']]\n",
    "sum_result1 = pd.concat(frames, axis=1)\n",
    "sum_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = sum_result1.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "counter1=collections.Counter(flat_vals)\n",
    "print((counter1))\n",
    "print(len(counter1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same results for all without ext and all with ext\n",
    "- rows do not match\n",
    "- 9 values appear in all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first 5 returned docs no difference between weighted and unweighted for alpha = 0.6,  alpha = 0.8, 1 #if all weight the same is same as\n",
    "# if expansion would not exist\n",
    "# only tokens / tokens + ext\n",
    "# [(99, 0.048582995951417005), (380, 0.046610169491525424), (244, 0.04477611940298507), (89, 0.04371584699453552), (376, 0.04034065441506051)]\n",
    "# [(244, 0.08955223880597014), (243, 0.08), (903, 0.04964539007092198), (99, 0.048582995951417005), (380, 0.046610169491525424)]\n",
    "# unweighted with candidate exp:\n",
    "# [(565, 0.048730964467005075), (1219, 0.0461864406779661), (12, 0.04042348411934552), (226, 0.039756782039289056), (22, 0.03749147920927062)]\n",
    "# [(565, 0.05177664974619289), (1219, 0.04745762711864407), (12, 0.04138594802694899), (226, 0.04069223573433115), (22, 0.03953646898432175)]\n",
    "# [(99, 0.048582995951417005), (380, 0.046610169491525424), (244, 0.04477611940298507), (89, 0.04371584699453552), (376, 0.04034065441506051)]\n",
    "# [(244, 0.08955223880597014), (243, 0.08), (903, 0.04964539007092198), (99, 0.048582995951417005), (380, 0.046610169491525424)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 10\n",
    "# alpha 0.8, 1\n",
    "# [(161, 0.027947874459039665), (313, 0.027129979796553974), (73, 0.025445292620865142), (402, 0.022429906542056073)]\n",
    "# [(161, 0.027915369391449767), (313, 0.02712754175646111), (73, 0.025570205421714953), (402, 0.022429906542056073)]\n",
    "# [(243, 0.032), (925, 0.031578947368421054), (1212, 0.03118536197295147), (910, 0.031168831168831172), (108, 0.03114754098360656)]\n",
    "# [(1212, 0.036276849642004776), (89, 0.034972677595628415), (376, 0.032989690721649485), (925, 0.031578947368421054), (910, 0.031168831168831172)]\n",
    "# for alpha 0.6 one change in one case\n",
    "# only tokens/tokens+ext\n",
    "# [(243, 0.04), (925, 0.039473684210526314), (1212, 0.03898170246618934), (910, 0.03896103896103896), (108, 0.0389344262295082)]\n",
    "# [(1212, 0.045346062052505964), (89, 0.04371584699453552), (376, 0.041237113402061855), (925, 0.039473684210526314), (910, 0.03896103896103896)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 2.TFIDF evaluation\n",
    "# texts_keys = []\n",
    "# texts_values = []\n",
    "# for key in sorted(texts.keys()) :\n",
    "#     texts_keys.append(key)\n",
    "#     texts_values.append(texts[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "# vectors_t = vectorizer.fit_transform(texts_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the first vector out (for the first document)\n",
    "# vector_t = vectors_t[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # place tf-idf values in a pandas data frame\n",
    "# vector_dframe_t = pd.DataFrame(vector_t.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "# vector_dframe_t = vector_dframe_t.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_dframe_t.head(50) #treaty ne sesteje lepo, lahko bi text olepsal preden gre v vectorizer, a pojavitev pomeni istost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to solve with transformation into string of tokenized text:\n",
    "# strings_keys = []\n",
    "# strings = []\n",
    "# for key in sorted(tokenized_docs.keys()) :\n",
    "#     strings_keys.append(key)\n",
    "#     list_tokens = tokenized_docs[key]\n",
    "#     corrected = \" \".join(list_tokens)\n",
    "#     strings.append(corrected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = vectorizer.fit_transform(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the first vector out (for the first document)\n",
    "# vector = vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # place tf-idf values in a pandas data frame\n",
    "# vector_dframe = pd.DataFrame(vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "# vector_dframe = vector_dframe.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_dframe.head(50) #not much difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tfidf only for query words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nb_docs_token_appears(tokens,tokenized_docs,texts):\n",
    "    nb_docs_token_appeared = []\n",
    "    for i in range(len(tokens)):\n",
    "        nb_docs_token_appeared.append(0)\n",
    "    for k, v in tokenized_docs.items():\n",
    "        text = texts.get(k)\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i]\n",
    "            if token in text:\n",
    "                nb_docs_token_appeared[i] = nb_docs_token_appeared[i]+1\n",
    "    return nb_docs_token_appeared\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_sum(tokens,tokenized_docs, texts):\n",
    "    nb_docs_token_appears =calculate_nb_docs_token_appears(tokens,tokenized_docs,texts)\n",
    "    l = len(tokenized_docs)\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        text = texts.get(k)\n",
    "        probability = 0\n",
    "        for i in range(len(tokens)):\n",
    "            token_frequecy = text.count(token[i])\n",
    "            idf = l/nb_docs_token_appears[i]\n",
    "            probability = probability+((token_frequency/n)*idf)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do\n",
    "def tfidf_sum_weights(original_tokens, top_expansion,tokenized_docs,texts, wv, alpha): \n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 0\n",
    "        for token in original_tokens+top_expansion:\n",
    "            token_frequency = texts.get(k).count(token)\n",
    "            probability = probability+(token_frequency/n)*word_value(token, alpha, original_tokens, top_expansion, wv)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
